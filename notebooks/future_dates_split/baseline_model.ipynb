{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e96c5a-5973-4b35-b192-9189c6e27134",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7773f512-8a2f-4bc5-aff7-ee92ae853f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, roc_auc_score, roc_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.exceptions import FitFailedWarning, ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from scripts.modeling_preparation.snap_modeling_prep import SnapProcessor\n",
    "from scripts.modeling_preparation.social_modeling_prep import SocialProcessor\n",
    "from scripts.config import SAVED_EMBEDDINGS_SNAP_DIR, PROCESSED_SNAP_MESSAGING_DIR, PROCESSED_SNAP_EMA_DIR\n",
    "from scripts.config import SAVED_EMBEDDINGS_SOCIAL_DIR, PROCESSED_SOCIAL_MESSAGING_DIR, PROCESSED_SOCIAL_EMA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517c762-cf17-4bc8-bd42-8a982ccb6841",
   "metadata": {},
   "source": [
    "### Instantiating the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf950db-c351-49cf-8481-d5a7d980240b",
   "metadata": {},
   "source": [
    "Utilizing the following features:\n",
    "\n",
    "      1. AvgWordCount\n",
    "      2. AvgResponseTime\n",
    "      3. MessageCount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896f2d6-52c3-4c1c-98f4-22acfe05a8e6",
   "metadata": {},
   "source": [
    "#### SNAP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "818ebdcb-a9f0-4aee-9759-8744ecd8f7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SnapProcessor(\n",
    "    model_name='distilbert/distilbert-base-uncased', \n",
    "    relationship='Romantic Partner', \n",
    "    save_dir=SAVED_EMBEDDINGS_SNAP_DIR,\n",
    "    embedding_method='mean_pooling'\n",
    ")\n",
    "\n",
    "snap_df = processor.prepare_modeling_df(\n",
    "    messaging_dir=PROCESSED_SNAP_MESSAGING_DIR, \n",
    "    ema_dir=PROCESSED_SNAP_EMA_DIR, \n",
    "    message_count=True,\n",
    "    embeddings=False, \n",
    "    sentiment_analysis=False,\n",
    "    return_train_test=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c351ab9-4b49-4c63-a6f7-306c5dd95947",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b53f753-4a40-4e43-99f2-4b92ede247b5",
   "metadata": {},
   "source": [
    "### Creating the test split based on future dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a98d01-1260-451b-b553-9f65ad077e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_df_sorted = snap_df.sort_values(by=['Participant ID','Date'])\n",
    "snap_df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1429b452-3b60-46ca-85f0-021e89e1b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrames to hold the training and test data\n",
    "snap_train_df = pd.DataFrame()\n",
    "snap_test_df = pd.DataFrame()\n",
    "\n",
    "# Step 2: Group by 'Participant ID' and split each group\n",
    "split_ratio = 0.8\n",
    "\n",
    "for participant_id, group in snap_df_sorted.groupby('Participant ID'):\n",
    "    group = group.sort_values(by='Date')  # Ensure the group is sorted by date\n",
    "    \n",
    "    if len(group) > 1:  # Only include in the test set if more than 1 day of data\n",
    "        split_point = int(len(group) * split_ratio)\n",
    "        \n",
    "        # Training data will be the earlier dates\n",
    "        train_participant = group.iloc[:split_point]\n",
    "        \n",
    "        # Test data will be the later dates\n",
    "        test_participant = group.iloc[split_point:]\n",
    "        \n",
    "        # Append to the training and test DataFrames\n",
    "        snap_train_df = pd.concat([snap_train_df, train_participant])\n",
    "        snap_test_df = pd.concat([snap_test_df, test_participant])\n",
    "    else:\n",
    "        # If only one day of data, include in the test set only\n",
    "        snap_test_df = pd.concat([snap_test_df, group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20da38-f331-4e0b-940e-a65b41ca9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_train_df['Disagreement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bfe1a-fc32-4838-90b5-09c24809a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_test_df['Disagreement'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38603f1b-197f-4cb2-9b1d-7a22047b6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_train_df['Participant ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82242d7f-b078-48b4-9f7f-eef22c0de49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_test_df['Participant ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d51474-1541-4c83-b9d6-97a0eacd9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424204ec-4267-4656-9d47-b46db8514486",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699008f-390b-4ba0-8982-e5b10ccec617",
   "metadata": {},
   "source": [
    "### Storing approach results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe1aadf3-8f02-44ee-9a53-66544aacc042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store all results for this approach\n",
    "approach_results = {\n",
    "    'Internal Validation': {\n",
    "        'Logistic Regression': {},\n",
    "        'Decision Tree': {},\n",
    "        'Random Forest': {},\n",
    "        'XGBoost': {}\n",
    "    },\n",
    "    'External Validation': {\n",
    "        'Logistic Regression': {},\n",
    "        'Decision Tree': {},\n",
    "        'Random Forest': {},\n",
    "        'XGBoost': {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786fdd2-8f68-44f3-ab1e-ff5ccc13b57a",
   "metadata": {},
   "source": [
    "### Internal validation (20% of the SNAP Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcda7e75-56a2-4d2b-b2a6-4078ef650048",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = snap_train_df.drop(['Participant ID', 'Date','Disagreement'],axis = 1)\n",
    "y_train = snap_train_df['Disagreement']\n",
    "\n",
    "X_test = snap_test_df.drop(['Participant ID', 'Date','Disagreement'],axis = 1)\n",
    "y_test = snap_test_df['Disagreement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e20954-cbb2-4555-9111-219af37fe99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d8e9349-b649-42d4-b5c9-0906260394b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "train_index = X_train.index\n",
    "test_index = X_test.index\n",
    "\n",
    "# Convert the scaled array back to a DataFrame\n",
    "X_train = pd.DataFrame(X_train_scaled, index=train_index, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, index=test_index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667580e-4813-4fd8-9da6-d789b88eabd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, auc\n",
    "\n",
    "def compute_class_weights(y_train):\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    return dict(enumerate(class_weights))\n",
    "\n",
    "def fit_and_return_results(estimator, param_grid, seeds):\n",
    "    balanced_accuracies = []\n",
    "    auc_scores = []\n",
    "    f1_macro_scores = []\n",
    "    recall_1_scores = []\n",
    "    classification_reports = []\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weights(y_train)\n",
    "    \n",
    "    # If the estimator supports class_weight, set it\n",
    "    if 'class_weight' in estimator.get_params():\n",
    "        estimator.set_params(class_weight=class_weights)\n",
    "\n",
    "    # Perform GridSearchCV for hyperparameter tuning with balanced accuracy\n",
    "    print(\"Performing hyperparameter tuning...\")\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "        warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "        grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5, n_jobs=-1, scoring='balanced_accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(grid_search.get_params())\n",
    "    \n",
    "    # Print the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Use the best model from GridSearchCV\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Use the best hyperparameters for evaluation across multiple seeds\n",
    "    for seed in seeds:\n",
    "        print(f\"\\nUsing random seed: {seed}\")\n",
    "\n",
    "        # Set the random state for reproducibility\n",
    "        if 'random_state' in best_model.get_params().keys():\n",
    "            best_model.set_params(random_state=seed)\n",
    "        \n",
    "        # Train the model on the current seed's training data\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        print(best_model.get_params)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model using balanced accuracy\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        balanced_accuracies.append(balanced_accuracy)\n",
    "        print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "\n",
    "        # Evaluate the model using f1-score(Macro)\n",
    "        f1_macro_score = f1_score(y_test, y_pred, average='macro')\n",
    "        f1_macro_scores.append(f1_macro_score)\n",
    "        print(\"F-1 Score(Macro):\", f1_macro_score)\n",
    "\n",
    "        # Evaluate the model using recall(Class 1)\n",
    "        recall_1_score = recall_score(y_test, y_pred, pos_label=1)\n",
    "        recall_1_scores.append(recall_1_score)\n",
    "        print(\"Recall (Class 1):\", recall_1_score)\n",
    "        \n",
    "        # Print the classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        classification_reports.append(report)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Calculate AUC\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        auc_scores.append(auc_score)\n",
    "        print(f\"AUC Score: {auc_score}\")\n",
    "\n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_balanced_accuracy = np.mean(balanced_accuracies)\n",
    "    avg_auc_score = np.mean(auc_scores)\n",
    "    avg_f1_macro_score = np.mean(f1_macro_scores)\n",
    "    avg_recall_1_score = np.mean(recall_1_scores)\n",
    "\n",
    "    print(f\"\\nAverage Balanced Accuracy: {avg_balanced_accuracy}\")\n",
    "    print(f\"\\nAverage AUC Score: {avg_auc_score}\")\n",
    "    print(f\"Average F-1 Score (Macro) : {avg_f1_macro_score}\")\n",
    "    print(f\"Average Recall Score(Class 1) : {avg_recall_1_score}\")\n",
    "\n",
    "    # Combine classification reports\n",
    "    avg_classification_report = {}\n",
    "    for label in classification_reports[0].keys():\n",
    "        if isinstance(classification_reports[0][label], dict):\n",
    "            avg_classification_report[label] = {}\n",
    "            for metric in classification_reports[0][label].keys():\n",
    "                avg_classification_report[label][metric] = np.mean([report[label][metric] for report in classification_reports])\n",
    "        else:\n",
    "            avg_classification_report[label] = np.mean([report[label] for report in classification_reports])\n",
    "    \n",
    "    # Convert average classification report to DataFrame\n",
    "    avg_classification_df = pd.DataFrame(avg_classification_report).transpose()\n",
    "    \n",
    "    # Display the averaged classification report in the regular format\n",
    "    avg_report_str = \"Classification Report (Avg. scores across 10 runs)\\n\"\n",
    "    avg_report_str += f\"{'':<15}{'precision':<15}{'recall':<15}{'f1-score':<15}{'support':<15}\\n\\n\"\n",
    "    for label, metrics in avg_classification_report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            avg_report_str += f\"{label:<15}{metrics['precision']:<15.2f}{metrics['recall']:<15.2f}{metrics['f1-score']:<15.2f}{metrics['support']:<15.0f}\\n\"\n",
    "        else:\n",
    "            if label == 'accuracy':\n",
    "                avg_report_str += f\"\\n{label:<45}{metrics:<15.2f}\\n\"\n",
    "            else:\n",
    "                avg_report_str += f\"{label:<15}{metrics:<15.2f}\\n\"\n",
    "    print(avg_report_str)\n",
    "\n",
    "    # Plot the results\n",
    "    # Calculate means and standard deviations\n",
    "    mean_balanced_accuracy = np.mean(balanced_accuracies)\n",
    "    std_balanced_accuracy = np.std(balanced_accuracies)\n",
    "    \n",
    "    mean_f1_macro = np.mean(f1_macro_scores)\n",
    "    std_f1_macro = np.std(f1_macro_scores)\n",
    "    \n",
    "    mean_recall_1 = np.mean(recall_1_scores)\n",
    "    std_recall_1 = np.std(recall_1_scores)\n",
    "    \n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    \n",
    "    # Metrics and their corresponding means and standard deviations\n",
    "    metrics = ['Balanced Accuracy', 'F1-Score (Macro)', 'Recall (Class 1)', 'AUC ROC Score']\n",
    "    means = [mean_balanced_accuracy, mean_f1_macro, mean_recall_1, mean_auc]\n",
    "    stds = [std_balanced_accuracy, std_f1_macro, std_recall_1, std_auc]\n",
    "    \n",
    "    colors = ['red', 'green', 'blue', 'orange']\n",
    "\n",
    "    # Plotting the bar chart with flipped axes and different colors\n",
    "    plt.figure(figsize=(7, 2))\n",
    "    bars = plt.barh(metrics, means, xerr=stds, capsize=5, color=colors, height = 0.3)\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.xlabel('Mean Score')\n",
    "    plt.title('Mean and Standard Deviation of Metrics After 10 Runs')\n",
    "    plt.xlim(0, 1)  # assuming all metrics are in the range [0, 1]\n",
    "\n",
    "    # Adjusting subplot to reduce spacing\n",
    "    plt.subplots_adjust(left=0.2, right=0.8, top=0.9, bottom=0.1)\n",
    "    \n",
    "    # Adding text labels with mean and standard deviation values\n",
    "    for bar, mean, std, color in zip(bars, means, stds, colors):\n",
    "        xval = bar.get_width()\n",
    "        offset = std * 1.2  # Dynamic adjustment based on the std value\n",
    "        plt.text(xval + offset + 0.01, bar.get_y() + bar.get_height() / 2, f'{mean:.2f} ± {std:.2f}', va='center', ha='left', color=color)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    key_metric_results = {\n",
    "        'Balanced Accuracy': (mean_balanced_accuracy, std_balanced_accuracy),\n",
    "        'F1-Score (Macro)': (mean_f1_macro, std_f1_macro),\n",
    "        'Recall (Class 1)': (mean_recall_1, std_recall_1),\n",
    "        'AUC ROC Score': (mean_auc, std_auc)\n",
    "    }\n",
    "    return key_metric_results\n",
    "\n",
    "# Generate a list of random seeds\n",
    "base_seed = 42\n",
    "random.seed(base_seed)\n",
    "seeds = [random.randint(1, 1000) for _ in range(10)]\n",
    "print(\"Random Seeds:\", seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e401d4bd-5229-4863-b41b-ed26d7f44f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63eeb3-35fd-4b39-9aea-0b70c3732b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y = y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b40ed-3873-4acc-948d-58064bcade74",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89620fb-0ff7-4fd0-a803-a4e2f4151447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state = 42)\n",
    "approach_results['Internal Validation']['Logistic Regression'] = fit_and_return_results(estimator = lr, param_grid = param_grid_lr,\n",
    "                                                                                        seeds = seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e84b23-26fd-434a-85ba-941395a05981",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee51ed-f657-42c7-9631-d4f167b435fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 42)\n",
    "approach_results['Internal Validation']['Decision Tree'] = fit_and_return_results(estimator = dtc, param_grid = param_grid_dt,\n",
    "                                                                                        seeds = seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f099f-7867-48b9-b738-0a56f77485fd",
   "metadata": {},
   "source": [
    "#### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668cd39a-1717-4eda-9ecb-315c387e98ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "approach_results['Internal Validation']['Random Forest'] = fit_and_return_results(estimator = rf, param_grid = param_grid_rf, seeds = seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3ac125-ac93-4091-a595-26d9cd229ed5",
   "metadata": {},
   "source": [
    "#### 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be6c8c-9a72-480d-b63f-f49daa44a7a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500], \n",
    "    'max_depth': [3, 5],  \n",
    "    'learning_rate': [0.1, 0.2],  \n",
    "    'subsample': [0.8, 1.0],  \n",
    "    'colsample_bytree': [0.8, 1.0],  \n",
    "    'gamma': [0, 0.1],\n",
    "    'scale_pos_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "approach_results['Internal Validation']['XGBoost'] = fit_and_return_results(estimator = xgb_model, param_grid = param_grid_xgb,\n",
    "                                                                                  seeds = seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b265c549-9f2d-448e-923c-4d818fcfdf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(approach_results,indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311ec10a-fe02-45de-aed1-6b63d2ac1d78",
   "metadata": {},
   "source": [
    "### External Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "558cac51-81af-4170-b633-072a502c7186",
   "metadata": {},
   "outputs": [],
   "source": [
    "snap_processor = SnapProcessor(\n",
    "    relationship='Romantic Partner'\n",
    ")\n",
    "\n",
    "snap_df = snap_processor.prepare_modeling_df(\n",
    "    messaging_dir=PROCESSED_SNAP_MESSAGING_DIR, \n",
    "    ema_dir=PROCESSED_SNAP_EMA_DIR, \n",
    "    message_count=True,\n",
    "    embeddings = False\n",
    ")\n",
    "\n",
    "\n",
    "social_processor = SocialProcessor(\n",
    "    relationship='Romantic Partner', \n",
    ")\n",
    "\n",
    "social_df = social_processor.prepare_modeling_df(\n",
    "    messaging_dir=PROCESSED_SOCIAL_MESSAGING_DIR, \n",
    "    ema_dir=PROCESSED_SOCIAL_EMA_DIR, \n",
    "    message_count=True,\n",
    "    embeddings = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e273d25-0e21-4756-afff-40045dfbb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining train and test variables\n",
    "X_train = snap_df.drop(['Participant ID', 'Date','Disagreement'], axis=1)\n",
    "y_train = snap_df['Disagreement']\n",
    "\n",
    "# External test set\n",
    "X_test = social_df.drop(['Participant ID', 'Date', 'Disagreement'], axis=1)\n",
    "y_test = social_df['Disagreement']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca6703-2b6c-4887-a09c-45fa2407a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5a1a52d-f484-47cf-a3bc-4b159f66ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "train_index = X_train.index\n",
    "test_index = X_test.index\n",
    "\n",
    "# Convert the scaled array back to a DataFrame\n",
    "X_train = pd.DataFrame(X_train_scaled, index=train_index, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, index=test_index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd45700-fe38-4c3f-92c2-0939a2de31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(y_train):\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    return dict(enumerate(class_weights))\n",
    "\n",
    "def fit_and_return_results(estimator, param_grid, seeds):\n",
    "    balanced_accuracies = []\n",
    "    auc_scores = []\n",
    "    f1_macro_scores = []\n",
    "    recall_1_scores = []\n",
    "    classification_reports = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"\\nUsing random seed: {seed}\")\n",
    "        \n",
    "        # Compute class weights\n",
    "        class_weights = compute_class_weights(y_train)\n",
    "        \n",
    "        # If the estimator supports class_weight, set it\n",
    "        if 'class_weight' in estimator.get_params():\n",
    "            estimator.set_params(class_weight=class_weights)\n",
    "\n",
    "        # Perform GridSearchCV for hyperparameter tuning with balanced accuracy\n",
    "        print(\"Performing hyperparameter tuning...\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", FitFailedWarning)\n",
    "            warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "            grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=5, n_jobs=-1, scoring='balanced_accuracy')\n",
    "            grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Print the best hyperparameters\n",
    "        best_params = grid_search.best_params_\n",
    "        print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "        # Use the best model from GridSearchCV\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "        # # Set the random state for reproducibility in the best model\n",
    "        if 'random_state' in best_model.get_params():\n",
    "            best_model.set_params(random_state=seed)\n",
    "\n",
    "        # Train the model on the current seed's training data\n",
    "        best_model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Evaluate the model using balanced accuracy\n",
    "        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "        balanced_accuracies.append(balanced_accuracy)\n",
    "        print(\"Balanced Accuracy:\", balanced_accuracy)\n",
    "\n",
    "        # Evaluate the model using f1-score(Macro)\n",
    "        f1_macro_score = f1_score(y_test, y_pred, average='macro')\n",
    "        f1_macro_scores.append(f1_macro_score)\n",
    "        print(\"F-1 Score(Macro):\", f1_macro_score)\n",
    "\n",
    "        # Evaluate the model using recall(Class 1)\n",
    "        recall_1_score = recall_score(y_test, y_pred, pos_label=1)\n",
    "        recall_1_scores.append(recall_1_score)\n",
    "        print(\"Recall (Class 1):\", recall_1_score)\n",
    "        \n",
    "        # Print the classification report\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        classification_reports.append(report)\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        # Calculate AUC\n",
    "        y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        auc_scores.append(auc_score)\n",
    "        print(f\"AUC Score: {auc_score}\")\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_balanced_accuracy = np.mean(balanced_accuracies)\n",
    "    avg_auc_score = np.mean(auc_scores)\n",
    "    avg_f1_macro_score = np.mean(f1_macro_scores)\n",
    "    avg_recall_1_score = np.mean(recall_1_scores)\n",
    "\n",
    "    print(f\"\\nAverage Balanced Accuracy: {avg_balanced_accuracy}\")\n",
    "    print(f\"Average AUC Score: {avg_auc_score}\")\n",
    "    print(f\"Average F-1 Score (Macro) : {avg_f1_macro_score}\")\n",
    "    print(f\"Average Recall Score(Class 1) : {avg_recall_1_score}\")\n",
    "\n",
    "    # Combine classification reports\n",
    "    avg_classification_report = {}\n",
    "    for label in classification_reports[0].keys():\n",
    "        if isinstance(classification_reports[0][label], dict):\n",
    "            avg_classification_report[label] = {}\n",
    "            for metric in classification_reports[0][label].keys():\n",
    "                avg_classification_report[label][metric] = np.mean([report[label][metric] for report in classification_reports])\n",
    "        else:\n",
    "            avg_classification_report[label] = np.mean([report[label] for report in classification_reports])\n",
    "    \n",
    "    # Convert average classification report to DataFrame\n",
    "    avg_classification_df = pd.DataFrame(avg_classification_report).transpose()\n",
    "    \n",
    "    # Display the averaged classification report in the regular format\n",
    "    avg_report_str = \"Classification Report (Avg. scores across 10 runs)\\n\"\n",
    "    avg_report_str += f\"{'':<15}{'precision':<15}{'recall':<15}{'f1-score':<15}{'support':<15}\\n\\n\"\n",
    "    for label, metrics in avg_classification_report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            avg_report_str += f\"{label:<15}{metrics['precision']:<15.2f}{metrics['recall']:<15.2f}{metrics['f1-score']:<15.2f}{metrics['support']:<15.0f}\\n\"\n",
    "        else:\n",
    "            if label == 'accuracy':\n",
    "                avg_report_str += f\"\\n{label:<45}{metrics:<15.2f}\\n\"\n",
    "            else:\n",
    "                avg_report_str += f\"{label:<15}{metrics:<15.2f}\\n\"\n",
    "    print(avg_report_str)\n",
    "\n",
    "    # Plot the results\n",
    "    # Calculate means and standard deviations\n",
    "    mean_balanced_accuracy = np.mean(balanced_accuracies)\n",
    "    std_balanced_accuracy = np.std(balanced_accuracies)\n",
    "    \n",
    "    mean_f1_macro = np.mean(f1_macro_scores)\n",
    "    std_f1_macro = np.std(f1_macro_scores)\n",
    "    \n",
    "    mean_recall_1 = np.mean(recall_1_scores)\n",
    "    std_recall_1 = np.std(recall_1_scores)\n",
    "    \n",
    "    mean_auc = np.mean(auc_scores)\n",
    "    std_auc = np.std(auc_scores)\n",
    "    \n",
    "    # Metrics and their corresponding means and standard deviations\n",
    "    metrics = ['Balanced Accuracy', 'F1-Score (Macro)', 'Recall (Class 1)', 'AUC ROC Score']\n",
    "    means = [mean_balanced_accuracy, mean_f1_macro, mean_recall_1, mean_auc]\n",
    "    stds = [std_balanced_accuracy, std_f1_macro, std_recall_1, std_auc]\n",
    "    \n",
    "    colors = ['red', 'green', 'blue', 'orange']\n",
    "\n",
    "    # Plotting the bar chart with flipped axes and different colors\n",
    "    plt.figure(figsize=(7, 2))\n",
    "    bars = plt.barh(metrics, means, xerr=stds, capsize=5, color=colors, height = 0.3)\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.xlabel('Mean Score')\n",
    "    plt.title('Mean and Standard Deviation of Metrics After 10 Runs')\n",
    "    plt.xlim(0, 1)  # assuming all metrics are in the range [0, 1]\n",
    "\n",
    "    # Adjusting subplot to reduce spacing\n",
    "    plt.subplots_adjust(left=0.2, right=0.8, top=0.9, bottom=0.1)\n",
    "    \n",
    "    # Adding text labels with mean and standard deviation values\n",
    "    for bar, mean, std, color in zip(bars, means, stds, colors):\n",
    "        xval = bar.get_width()\n",
    "        offset = std * 1.2  # Dynamic adjustment based on the std value\n",
    "        plt.text(xval + offset + 0.01, bar.get_y() + bar.get_height() / 2, f'{mean:.2f} ± {std:.2f}', va='center', ha='left', color=color)\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "    key_metric_results = {\n",
    "        'Balanced Accuracy': (mean_balanced_accuracy, std_balanced_accuracy),\n",
    "        'F1-Score (Macro)': (mean_f1_macro, std_f1_macro),\n",
    "        'Recall (Class 1)': (mean_recall_1, std_recall_1),\n",
    "        'AUC ROC Score': (mean_auc, std_auc)\n",
    "    }\n",
    "    return key_metric_results\n",
    "\n",
    "\n",
    "# Generate a list of random seeds\n",
    "base_seed = 42\n",
    "random.seed(base_seed)\n",
    "seeds = [random.randint(1, 1000) for _ in range(10)]\n",
    "print(\"Random Seeds:\", seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e70c818-29c6-4560-acb4-7b1b5c35dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y = y_train)\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264b317-c02b-4381-92f7-d843c876bc53",
   "metadata": {},
   "source": [
    "#### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011be54-ab6c-4a06-b7e5-2ad2be89c3d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression(random_state=42)\n",
    "approach_results['External Validation']['Logistic Regression'] = fit_and_return_results(estimator = lr, param_grid = param_grid_lr,\n",
    "                                                                                        seeds = seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4bbef7-c4a7-4f1f-918f-3922447065c1",
   "metadata": {},
   "source": [
    "#### 2. Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405967a5-3a96-4274-95e7-3cdb6153bc39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "dtc = DecisionTreeClassifier(random_state = 42)\n",
    "approach_results['External Validation']['Decision Tree'] = fit_and_return_results(estimator = dtc, param_grid = param_grid_dt, seeds = seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330c34f-cc4d-40f5-9712-68af26eea0ef",
   "metadata": {},
   "source": [
    "#### 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5869c76a-7b71-4356-a223-da8e9105d687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "approach_results['External Validation']['Random Forest'] = fit_and_return_results(estimator = rf, param_grid = param_grid_rf, seeds = seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f67b90-bea9-4827-acc3-b340922f6510",
   "metadata": {},
   "source": [
    "#### 4. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef4974-7f46-41e6-94e6-9328ca38a254",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 200, 500], \n",
    "    'max_depth': [3, 5],  \n",
    "    'learning_rate': [0.1, 0.2],  \n",
    "    'subsample': [0.8, 1.0],  \n",
    "    'colsample_bytree': [0.8, 1.0],  \n",
    "    'gamma': [0, 0.1],\n",
    "    'scale_pos_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(random_state = 42)\n",
    "approach_results['External Validation']['XGBoost'] = fit_and_return_results(estimator = xgb_model, param_grid = param_grid_xgb, seeds = seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099d80c-06f1-48a7-9b0a-99b315d1afdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(json.dumps(approach_results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b75359-2d23-430c-bb5d-42f89df303d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
